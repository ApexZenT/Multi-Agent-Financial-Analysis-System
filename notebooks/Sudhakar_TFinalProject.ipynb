{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ec5b1bd",
      "metadata": {
        "id": "1ec5b1bd"
      },
      "source": [
        "\n",
        "# Multi-Agent Financial Analysis System\n",
        "\n",
        "Github Link: https://github.com/ApexZenT/Course_8_NLP/tree/master/src/Week7/Multi-Agent%20Financial%20Analysis%20System/FinalProject\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1t--PvcfzYa",
        "outputId": "4423c98f-8224-4a80-e46a-5507a8e12a17"
      },
      "id": "n1t--PvcfzYa",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt4all in /usr/local/lib/python3.12/dist-packages (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from gpt4all) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->gpt4all) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->gpt4all) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->gpt4all) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->gpt4all) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "376aeb81",
      "metadata": {
        "id": "376aeb81"
      },
      "outputs": [],
      "source": [
        "# --- Standard Library ---\n",
        "import logging\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from dateutil import parser\n",
        "from typing import List, Optional\n",
        "\n",
        "# --- Third-party Libraries ---\n",
        "import requests\n",
        "import polars as pl\n",
        "import yfinance as yf\n",
        "from dataclasses import dataclass\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime\n",
        "from sqlalchemy.orm import declarative_base, sessionmaker\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "89f9a010",
      "metadata": {
        "id": "89f9a010"
      },
      "outputs": [],
      "source": [
        "# Global settings for the Multi-Agent Financial Analysis System\n",
        "\n",
        "\n",
        "# Set this flag to True after processing CSV and ingesting into DB to avoid re-processing\n",
        "load_csv_to_db='false'\n",
        "\n",
        "# News API configuration\n",
        "news_api_key='NEWS_API_KEY'\n",
        "news_api_endpoint_everything='https://newsapi.org/v2/everything'\n",
        "news_api_endpoint_top_headlines='https://newsapi.org/v2/top-headlines'\n",
        "\n",
        "# FRED API Configuration\n",
        "fred_api_key='FRES_API_KEY'\n",
        "fred_api_endpoint='https://api.stlouisfed.org/fred'\n",
        "\n",
        "# DB path\n",
        "news_db_path='/data/processed/news.db'\n",
        "\n",
        "# default limit for fetching news records\n",
        "default_news_fetch_limit='1000'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "eaadc6b8",
      "metadata": {
        "id": "eaadc6b8"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "BASE_PATH = Path('./data/processed')\n",
        "BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- News DB ---\n",
        "NEWS_DATABASE_URL = f\"sqlite:///{BASE_PATH}/news.db\"\n",
        "engine_news = create_engine(NEWS_DATABASE_URL, echo=False, connect_args={\"check_same_thread\": False})\n",
        "SessionNews = sessionmaker(bind=engine_news, expire_on_commit=False, autoflush=False)\n",
        "BaseNews = declarative_base()\n",
        "\n",
        "# --- Agent Memory DB ---\n",
        "AGENTMEMORY_DATABASE_URL = f\"sqlite:///{BASE_PATH}/agent_memory.db\"\n",
        "engine_agent_memory = create_engine(AGENTMEMORY_DATABASE_URL, echo=False, connect_args={\"check_same_thread\": False})\n",
        "SessionAgentMemory = sessionmaker(bind=engine_agent_memory, expire_on_commit=False, autoflush=False)\n",
        "BaseAgentMemory = declarative_base()\n",
        "\n",
        "# Initialize tables\n",
        "def init_db():\n",
        "    BaseNews.metadata.create_all(bind=engine_news)\n",
        "    BaseAgentMemory.metadata.create_all(bind=engine_agent_memory)\n",
        "    logger.info(\"Database tables created successfully\")\n",
        "\n",
        "init_db()  # Call once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "49db0ca4",
      "metadata": {
        "id": "49db0ca4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- ORM Models ---\n",
        "class News(BaseNews):\n",
        "    __tablename__ = 'news'\n",
        "    __table_args__ = {'extend_existing': True}\n",
        "    id = Column(Integer, primary_key=True, index=True)\n",
        "    title = Column(String, index=True)\n",
        "    description = Column(String)\n",
        "    source = Column(String, index=True)\n",
        "    url = Column(String, unique=True, index=True)\n",
        "    publishedAt = Column(DateTime, index=True)\n",
        "\n",
        "class AgentMemory(BaseAgentMemory):\n",
        "    __tablename__ = 'agent_memory'\n",
        "    __table_args__ = {'extend_existing': True}\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    agent_name = Column(String)\n",
        "    context = Column(String)\n",
        "    inputs = Column(Text)\n",
        "    output = Column(Text)\n",
        "    timestamp = Column(String)\n",
        "\n",
        "# --- DTOs ---\n",
        "\n",
        "@dataclass\n",
        "class NewsDTO:\n",
        "    title: str\n",
        "    publishedAt: Optional[datetime] = None\n",
        "    description: Optional[str] = None\n",
        "    source: Optional[str] = None\n",
        "    url: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class AgentMemoryDTO:\n",
        "    agent_name: str\n",
        "    context: str\n",
        "    inputs: str\n",
        "    output: str\n",
        "    timestamp: Optional[str] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a3607a59",
      "metadata": {
        "id": "a3607a59"
      },
      "outputs": [],
      "source": [
        "class CSVAdapter:\n",
        "    \"\"\"\n",
        "    Adapter for loading and normalizing financial news CSVs\n",
        "    (e.g., CNBC, Reuters, Bloomberg) into a common schema.\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_PATH = Path('./data/raw')\n",
        "\n",
        "    _rename_map = {\n",
        "        \"headline\": \"title\",\n",
        "        \"headlines\": \"title\",\n",
        "        \"content\": \"description\",\n",
        "        \"summary_text\": \"description\",\n",
        "        \"summary\": \"description\",\n",
        "        \"date_published\": \"publishedAt\",\n",
        "        \"timestamp\": \"publishedAt\",\n",
        "        \"time\": \"publishedAt\",\n",
        "    }\n",
        "\n",
        "    def fetch_news_from_csv(self) -> List[NewsDTO]:\n",
        "        all_news: List[NewsDTO] = []\n",
        "\n",
        "        csv_files = list(self.BASE_PATH.glob(\"*.csv\"))\n",
        "        if not csv_files:\n",
        "            logger.warning(\"No CSV files found in %s\", self.BASE_PATH)\n",
        "            return all_news\n",
        "\n",
        "        logger.info(\"Found %d CSV files to process.\", len(csv_files))\n",
        "\n",
        "        for csv_file in csv_files:\n",
        "            try:\n",
        "                df = pl.read_csv(csv_file)\n",
        "\n",
        "                # Standardize column names\n",
        "                df = df.rename(\n",
        "                    {\n",
        "                        col: col.replace(\"-\", \"_\").replace(\" \", \"_\").lower().strip()\n",
        "                        for col in df.columns\n",
        "                    }\n",
        "                )\n",
        "                df = df.rename(self._rename_map, strict=False)\n",
        "\n",
        "                filtered_df = df.filter(\n",
        "                    pl.col(\"title\").is_not_null() & (pl.col(\"title\") != \"\")\n",
        "                )\n",
        "\n",
        "                rows_as_dicts = filtered_df.to_dicts()\n",
        "                logger.info(\n",
        "                    \"Processing %d rows from CSV file: %s\",\n",
        "                    len(rows_as_dicts),\n",
        "                    csv_file.name,\n",
        "                )\n",
        "\n",
        "                for row in rows_as_dicts:\n",
        "                    news_item = NewsDTO(\n",
        "                        title=row.get(\"title\"),\n",
        "                        publishedAt=parse_datetime(row.get(\"publishedAt\")),\n",
        "                        description=row.get(\"description\"),\n",
        "                        source=csv_file.stem,  # use file name as source\n",
        "                        url=row.get(\"url\"),\n",
        "                    )\n",
        "                    all_news.append(news_item)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(\"Error processing CSV file %s: %s\", csv_file.name, e)\n",
        "\n",
        "        logger.info(\"Total news items fetched from CSVs: %d\", len(all_news))\n",
        "        return all_news\n",
        "\n",
        "\n",
        "class NewsAdapter:\n",
        "    \"\"\"Adapter for fetching and normalizing financial news from external APIs.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        logger.info(\"Initialized NewsAdapter with provided API key.\")\n",
        "\n",
        "    def fetch_news(self, endpoint: str, params: dict) -> List[NewsDTO]:\n",
        "        \"\"\"Fetch news from an API endpoint and return as list of NewsDTOs.\"\"\"\n",
        "        params[\"apiKey\"] = self.api_key\n",
        "        try:\n",
        "            response = requests.get(endpoint, params=params)\n",
        "            response.raise_for_status()\n",
        "            articles = self._parse_articles(response.json())\n",
        "            logger.info(\"Fetched %d articles from endpoint %s\", len(articles), endpoint)\n",
        "            return articles\n",
        "        except requests.RequestException as e:\n",
        "            logger.error(\"Request failed for endpoint %s: %s\", endpoint, e)\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            logger.error(\"Unexpected error while fetching news: %s\", e)\n",
        "            return []\n",
        "\n",
        "    def _parse_articles(self, response_json: dict) -> List[NewsDTO]:\n",
        "        \"\"\"Parse the JSON response into a list of NewsDTO objects.\"\"\"\n",
        "        articles = response_json.get(\"articles\", [])\n",
        "        news_list = []\n",
        "        for a in articles:\n",
        "            try:\n",
        "                news_item = NewsDTO(\n",
        "                    title=a.get(\"title\"),\n",
        "                    description=a.get(\"description\"),\n",
        "                    source=a.get(\"source\", {}).get(\"name\"),\n",
        "                    publishedAt=parse_datetime(a.get(\"publishedAt\")),\n",
        "                    url=a.get(\"url\"),\n",
        "                )\n",
        "                news_list.append(news_item)\n",
        "            except Exception as e:\n",
        "                logger.warning(\"Failed to parse article: %s\", e)\n",
        "        return news_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c190ec56",
      "metadata": {
        "id": "c190ec56"
      },
      "outputs": [],
      "source": [
        "# Utilities\n",
        "def save_to_db(session_factory: sessionmaker, orm_instance) -> bool:\n",
        "    \"\"\"\n",
        "    Generic utility to save a pre-built ORM instance to the database.\n",
        "\n",
        "    Args:\n",
        "        session_factory: SQLAlchemy sessionmaker\n",
        "        orm_instance: An instance of a SQLAlchemy ORM class\n",
        "\n",
        "    Returns:\n",
        "        bool: True if saved successfully, False otherwise\n",
        "    \"\"\"\n",
        "    with session_factory() as session:\n",
        "        try:\n",
        "            session.add(orm_instance)\n",
        "            session.commit()\n",
        "            logger.info(\"Saved ORM instance: %s\", orm_instance)\n",
        "            return True\n",
        "        except SQLAlchemyError as e:\n",
        "            session.rollback()\n",
        "            logger.error(\"Failed to save ORM instance: %s | Error: %s\", orm_instance, e)\n",
        "            return False\n",
        "\n",
        "\n",
        "# Map common timezone abbreviations to UTC offsets (seconds)\n",
        "TZINFOS = {\n",
        "    \"ET\": -5 * 3600,\n",
        "    \"EST\": -5 * 3600,\n",
        "    \"EDT\": -4 * 3600,\n",
        "    \"CT\": -6 * 3600,\n",
        "    \"CST\": -6 * 3600,\n",
        "    \"CDT\": -5 * 3600,\n",
        "    \"MT\": -7 * 3600,\n",
        "    \"MST\": -7 * 3600,\n",
        "    \"MDT\": -6 * 3600,\n",
        "    \"PT\": -8 * 3600,\n",
        "    \"PST\": -8 * 3600,\n",
        "    \"PDT\": -7 * 3600,\n",
        "}\n",
        "\n",
        "\n",
        "def parse_datetime(\n",
        "    dt_str: Optional[str], default_tz: timezone = timezone.utc\n",
        ") -> Optional[datetime]:\n",
        "    \"\"\"\n",
        "    Parse a datetime string into a Python datetime object.\n",
        "\n",
        "    Args:\n",
        "        dt_str: The string to parse.\n",
        "        default_tz: Timezone to assign if the string is naive.\n",
        "\n",
        "    Returns:\n",
        "        datetime object (aware) or None if input is None or invalid.\n",
        "    \"\"\"\n",
        "    if not dt_str:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        dt = parser.parse(dt_str, tzinfos=TZINFOS)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=default_tz)\n",
        "        # Normalize to UTC\n",
        "        dt = dt.astimezone(timezone.utc)\n",
        "        return dt\n",
        "    except (ValueError, TypeError) as e:\n",
        "        logger.warning(\"Could not parse datetime '%s': %s\", dt_str, e)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f068f623",
      "metadata": {
        "id": "f068f623"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NewsDB:\n",
        "    \"\"\"DAO / DAL for News table using SQLAlchemy ORM.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = SessionNews()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.close()\n",
        "\n",
        "    def insert_news(self, news_items: List[NewsDTO]):\n",
        "        \"\"\"Insert multiple news records into the database in batches, skipping duplicates.\"\"\"\n",
        "        if not news_items:\n",
        "            logger.info(\"No news items to process.\")\n",
        "            return\n",
        "\n",
        "        titles_set = {item.title for item in news_items if item.title}\n",
        "        titles_list = list(titles_set)\n",
        "\n",
        "        # Check for existing titles in batches\n",
        "        existing_titles = set()\n",
        "        logger.info(\"Checking for existing titles in batches...\")\n",
        "        chunk_size_lookup = 500\n",
        "        for i in range(0, len(titles_list), chunk_size_lookup):\n",
        "            batch_titles = titles_list[i : i + chunk_size_lookup]\n",
        "            existing_in_batch = (\n",
        "                self.session.query(News.title)\n",
        "                .filter(News.title.in_(batch_titles))\n",
        "                .all()\n",
        "            )\n",
        "            existing_titles.update(title for (title,) in existing_in_batch)\n",
        "        logger.info(\"Found %d existing articles.\", len(existing_titles))\n",
        "\n",
        "        # Prepare new records\n",
        "        new_records = [\n",
        "            {\n",
        "                \"title\": item.title,\n",
        "                \"publishedAt\": item.publishedAt,\n",
        "                \"description\": item.description,\n",
        "                \"source\": item.source,\n",
        "                \"url\": item.url,\n",
        "            }\n",
        "            for item in news_items\n",
        "            if item.title not in existing_titles\n",
        "        ]\n",
        "\n",
        "        if not new_records:\n",
        "            logger.info(\"No new articles to insert.\")\n",
        "            return\n",
        "\n",
        "        # Bulk insert in chunks\n",
        "        chunk_size_insert = 500\n",
        "        try:\n",
        "            for i in range(0, len(new_records), chunk_size_insert):\n",
        "                batch = new_records[i : i + chunk_size_insert]\n",
        "                self.session.bulk_insert_mappings(News, batch)\n",
        "                logger.info(\n",
        "                    \"Inserted batch %d with %d items.\",\n",
        "                    i // chunk_size_insert + 1,\n",
        "                    len(batch),\n",
        "                )\n",
        "            self.session.commit()\n",
        "            logger.info(\"All new news items inserted successfully.\")\n",
        "        except Exception as e:\n",
        "            self.session.rollback()\n",
        "            logger.error(\"Error during bulk insert: %s\", e)\n",
        "            raise\n",
        "\n",
        "    def fetch_news(\n",
        "        self, limit: int = 1000, source: Optional[str] = None, q: Optional[str] = None\n",
        "    ) -> List[NewsDTO]:\n",
        "        \"\"\"Fetch news records from the database.\"\"\"\n",
        "        query = self.session.query(News)\n",
        "        if source:\n",
        "            query = query.filter(News.source == source)\n",
        "        if q:\n",
        "            query = query.filter(News.title.ilike(f\"%{q}%\"))\n",
        "\n",
        "        records = query.limit(limit).all()\n",
        "        news_dtos = [\n",
        "            NewsDTO(\n",
        "                title=r.title,\n",
        "                publishedAt=r.publishedAt,\n",
        "                description=r.description,\n",
        "                source=r.source,\n",
        "                url=r.url,\n",
        "            )\n",
        "            for r in records\n",
        "        ]\n",
        "        logger.info(\"Fetched %d news records from DB.\", len(news_dtos))\n",
        "        return news_dtos\n",
        "\n",
        "    def close(self):\n",
        "        self.session.close()\n",
        "        logger.info(\"Database session closed.\")\n",
        "\n",
        "class NewsService:\n",
        "    \"\"\"Handles all business logic related to fetching and storing news.\"\"\"\n",
        "\n",
        "    load_limit: int = int(default_news_fetch_limit)\n",
        "    load_csv_flag: bool = (load_csv_to_db == True)\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.adapter = NewsAdapter(api_key)\n",
        "        self.db = NewsDB()\n",
        "\n",
        "        if self.load_csv_flag:\n",
        "            logger.info(\"CSV-to-DB load enabled. Starting initial load...\")\n",
        "            self._load_csv_to_db()\n",
        "        else:\n",
        "            logger.info(\"CSV-to-DB load disabled. Skipping initial load.\")\n",
        "\n",
        "    def _load_csv_to_db(self):\n",
        "        \"\"\"Private method to load CSV into DB only once.\"\"\"\n",
        "        csv_adapter = CSVAdapter()\n",
        "        news_records = csv_adapter.fetch_news_from_csv()\n",
        "        self.db.insert_news(news_records)\n",
        "        logger.info(\"Inserted %d CSV records into DB.\", len(news_records))\n",
        "\n",
        "    def fetch_and_store_news(self, endpoint: str, **kwargs) -> List[NewsDTO]:\n",
        "        \"\"\"Fetch news using flexible parameters and optionally store to DB.\"\"\"\n",
        "        logger.debug(\"Fetching news from endpoint: %s | Params: %s\", endpoint, kwargs)\n",
        "        news_items = self.adapter.fetch_news(endpoint, kwargs)\n",
        "        self.db.insert_news(news_items)\n",
        "        logger.info(\"Fetched and stored %d news items.\", len(news_items))\n",
        "        return news_items\n",
        "\n",
        "    def get_stored_news(self, **kwargs) -> List[NewsDTO]:\n",
        "        \"\"\"Fetch stored news with flexible filtering options.\"\"\"\n",
        "        limit = kwargs.get(\"limit\", self.load_limit)\n",
        "        logger.debug(\"Fetching %d stored news records from DB.\", limit)\n",
        "        news = self.db.fetch_news(limit=limit)\n",
        "        logger.info(\"Retrieved %d stored news records.\", len(news))\n",
        "        return news\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2ae6d94c",
      "metadata": {
        "id": "2ae6d94c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NewsTool:\n",
        "    \"\"\"Tool exposing financial news fetching & DB operations.\"\"\"\n",
        "\n",
        "    name = \"NewsTool\"\n",
        "    description = \"Fetch and store financial news via NewsAPI and local DB.\"\n",
        "\n",
        "    # Load configuration once\n",
        "    _api_key: str = news_api_key\n",
        "    _endpoint_everything: str = news_api_endpoint_everything\n",
        "    _endpoint_top_headlines: str = news_api_endpoint_top_headlines\n",
        "\n",
        "    _service: NewsService = None  # lazy init\n",
        "\n",
        "    @property\n",
        "    def service(self):\n",
        "        \"\"\"Initialize NewsService lazily.\"\"\"\n",
        "        if self._service is None:\n",
        "            if not self._api_key:\n",
        "                logger.error(\"Missing News API key in global-settings.properties\")\n",
        "                raise ValueError(\"Missing News API key in global-settings.properties\")\n",
        "            logger.info(\"Initializing NewsService...\")\n",
        "            self._service = NewsService(api_key=self._api_key)\n",
        "        return self._service\n",
        "\n",
        "    def fetch_everything(self, **kwargs) -> List[NewsDTO]:\n",
        "        logger.debug(\"Fetching everything news with params: %s\", kwargs)\n",
        "        news_items = self.service.fetch_and_store_news(\n",
        "            self._endpoint_everything, **kwargs\n",
        "        )\n",
        "        logger.info(\"Fetched %d 'everything' news items\", len(news_items))\n",
        "        return news_items\n",
        "\n",
        "    def fetch_top_headlines(self, **kwargs) -> List[NewsDTO]:\n",
        "        logger.debug(\"Fetching top headlines with params: %s\", kwargs)\n",
        "        news_items = self.service.fetch_and_store_news(\n",
        "            self._endpoint_top_headlines, **kwargs\n",
        "        )\n",
        "        logger.info(\"Fetched %d top headlines\", len(news_items))\n",
        "        return news_items\n",
        "\n",
        "    def fetch_news(self, source: str = \"recent\", **kwargs) -> List[NewsDTO]:\n",
        "        logger.info(\"Fetching news from source: '%s'\", source)\n",
        "        news: List[NewsDTO] = []\n",
        "\n",
        "        if source == \"recent\":\n",
        "            news.extend(self.fetch_top_headlines(**kwargs))\n",
        "        elif source == \"history\":\n",
        "            news.extend(self.fetch_everything(**kwargs))\n",
        "            news.extend(self.service.get_stored_news(**kwargs))\n",
        "        elif source == \"db\":\n",
        "            news.extend(self.service.get_stored_news(**kwargs))\n",
        "        else:\n",
        "            logger.error(\"Invalid news source '%s'\", source)\n",
        "            raise ValueError(f\"Invalid news source '{source}'\")\n",
        "\n",
        "        # Deduplicate by URL\n",
        "        seen = set()\n",
        "        unique_news = []\n",
        "        for article in news:\n",
        "            key = getattr(article, \"url\", None)\n",
        "            if key and key not in seen:\n",
        "                seen.add(key)\n",
        "                unique_news.append(article)\n",
        "\n",
        "        logger.info(\"Returning %d unique news articles\", len(unique_news))\n",
        "        return unique_news\n",
        "\n",
        "\n",
        "\n",
        "class StockTool:\n",
        "    \"\"\"A tool for fetching stock data using yfinance.\"\"\"\n",
        "\n",
        "    name = \"StockAPI\"\n",
        "    description = (\n",
        "        \"A tool for fetching stock data. \"\n",
        "        \"Use this tool to get historical stock prices, current stock prices, and other financial data.\"\n",
        "    )\n",
        "\n",
        "    def _get_ticker(self, symbol: str):\n",
        "        \"\"\"Helper method to validate and return a yfinance Ticker object.\"\"\"\n",
        "        try:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            if not ticker.info:\n",
        "                raise ValueError(f\"Ticker symbol '{symbol}' not found or has no info.\")\n",
        "            return ticker\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error fetching ticker '%s': %s\", symbol, e)\n",
        "            raise\n",
        "\n",
        "    def fetch_historical_data(\n",
        "        self, symbol: str, period: str = \"1yr\", interval: str = \"1d\"\n",
        "    ) -> dict:\n",
        "        \"\"\"Fetch historical stock data for a given ticker symbol.\"\"\"\n",
        "        logger.info(\n",
        "            \"Fetching historical data for %s | period=%s, interval=%s\",\n",
        "            symbol,\n",
        "            period,\n",
        "            interval,\n",
        "        )\n",
        "        ticker = self._get_ticker(symbol)\n",
        "        try:\n",
        "            history = ticker.history(period=period, interval=interval)\n",
        "            data = history.to_dict()\n",
        "            logger.debug(\n",
        "                \"Historical data fetched for %s: %d entries\", symbol, len(history)\n",
        "            )\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error fetching historical data for %s: %s\", symbol, e)\n",
        "            raise\n",
        "\n",
        "    def fetch_financial_info(self, symbol: str) -> dict:\n",
        "        \"\"\"Fetch financial statements for a given ticker symbol.\"\"\"\n",
        "        logger.info(\"Fetching financial info for %s\", symbol)\n",
        "        ticker = self._get_ticker(symbol)\n",
        "        try:\n",
        "            financials = {\n",
        "                \"income_statement\": ticker.financials,\n",
        "                \"balance_sheet\": ticker.balance_sheet,\n",
        "                \"cashflow\": ticker.cashflow,\n",
        "            }\n",
        "            logger.debug(\"Financial info fetched for %s\", symbol)\n",
        "            return financials\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error fetching financial info for %s: %s\", symbol, e)\n",
        "            raise\n",
        "\n",
        "    def fetch_symbol_info(self, symbol: str) -> dict:\n",
        "        \"\"\"Fetch general information for a given ticker symbol.\"\"\"\n",
        "        logger.info(\"Fetching symbol info for %s\", symbol)\n",
        "        ticker = self._get_ticker(symbol)\n",
        "        try:\n",
        "            info = ticker.info\n",
        "            data = {\n",
        "                \"name\": info.get(\"longName\"),\n",
        "                \"sector\": info.get(\"sector\"),\n",
        "                \"industry\": info.get(\"industry\"),\n",
        "                \"market_cap\": info.get(\"marketCap\"),\n",
        "                \"beta\": info.get(\"beta\"),\n",
        "                \"pe_ratio\": info.get(\"trailingPE\"),\n",
        "                \"dividend_yield\": info.get(\"dividendYield\"),\n",
        "            }\n",
        "            logger.debug(\"Symbol info fetched for %s: %s\", symbol, data)\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error fetching symbol info for %s: %s\", symbol, e)\n",
        "            raise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EconomicDataTool:\n",
        "    \"\"\"Tool for fetching economic indicators from FRED API.\"\"\"\n",
        "\n",
        "    name = \"EconomicDataTool\"\n",
        "    description = \"Fetch economic data (GDP, CPI, unemployment, etc.) via FRED API.\"\n",
        "\n",
        "    # Load configuration once\n",
        "    api_key: str = fred_api_key\n",
        "    fred_endpoint: str = fred_api_endpoint\n",
        "\n",
        "    def fetch_series(\n",
        "        self, series_id: str, start_date: str = None, end_date: str = None\n",
        "    ) -> dict:\n",
        "        \"\"\"\n",
        "        Fetch time series data from FRED.\n",
        "\n",
        "        Args:\n",
        "            series_id (str): Example 'GDP', 'CPIAUCSL', 'UNRATE'\n",
        "            start_date (str): 'YYYY-MM-DD'\n",
        "            end_date (str): 'YYYY-MM-DD'\n",
        "\n",
        "        Returns:\n",
        "            dict: Time series data\n",
        "        \"\"\"\n",
        "        params = {\"series_id\": series_id, \"api_key\": self.api_key, \"file_type\": \"json\"}\n",
        "        if start_date:\n",
        "            params[\"observation_start\"] = start_date\n",
        "        if end_date:\n",
        "            params[\"observation_end\"] = end_date\n",
        "\n",
        "        logger.info(\n",
        "            \"Fetching economic series '%s' from %s\", series_id, self.fred_endpoint\n",
        "        )\n",
        "        try:\n",
        "            response = requests.get(\n",
        "                f\"{self.fred_endpoint}/series/observations\", params=params, timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if \"observations\" not in data:\n",
        "                logger.error(\"No observations found for series %s: %s\", series_id, data)\n",
        "                raise ValueError(f\"Error fetching series {series_id}: {data}\")\n",
        "            logger.info(\n",
        "                \"Fetched %d observations for series %s\",\n",
        "                len(data[\"observations\"]),\n",
        "                series_id,\n",
        "            )\n",
        "            return data[\"observations\"]\n",
        "        except requests.RequestException as e:\n",
        "            logger.exception(\"Request failed for series %s: %s\", series_id, e)\n",
        "            raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "319f33d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "319f33d4",
        "outputId": "0b0faac5-1645-4487-ae54-ad3ff396987c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 2.18G/2.18G [00:26<00:00, 80.9MiB/s]\n",
            "Verifying: 100%|██████████| 2.18G/2.18G [00:07<00:00, 305MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Global model instance (loaded once and reused across agents)\n",
        "# -------------------------------------------------------------------\n",
        "MODEL = GPT4All(model_name=\"Phi-3-mini-4k-instruct.Q4_0.gguf\", device='cpu')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Setup module-level logger\n",
        "# -------------------------------------------------------------------\n",
        "logger = logging.getLogger(__name__)\n",
        "mock = True # To invoke without LLM ( for Debugging)\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"Base class for all AI agents in the multi-agent framework.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, name: str, role: str, model: str = \"gguf-model-falcon-q4_0.gguf\"\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.role = role\n",
        "        self.model = MODEL  # use the preloaded model instance\n",
        "        logger.info(\"Initialized agent: %s (%s)\", self.name, self.role)\n",
        "        self.task_counter = {}\n",
        "    # -------------------------------------------------------------------\n",
        "    # Core LLM call\n",
        "    # -------------------------------------------------------------------\n",
        "    def call_llm(self, prompt: str) -> str:\n",
        "        \"\"\"Send a prompt to the LLM and return its response.\"\"\"\n",
        "        if mock:\n",
        "            return f\"Mock response from {self.name}: {prompt[:50]}...\"\n",
        "        try:\n",
        "            result = self.model.generate(prompt)\n",
        "            logger.debug(\"LLM response for %s: %s...\", self.name, result[:80])\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(\"LLM call failed for %s: %s\", self.name, e, exc_info=True)\n",
        "            return f\"Mock response from {self.name}: {prompt[:50]}...\"\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Process & persist memory\n",
        "    # -------------------------------------------------------------------\n",
        "    def process(self, prompt: str, **kwargs) -> str:\n",
        "        \"\"\"Process a prompt through the agent and store its memory.\"\"\"\n",
        "        logger.info(\n",
        "            \"Agent %s processing context: %s\",\n",
        "            self.name,\n",
        "            kwargs.get(\"context\", \"UnknownContext\"),\n",
        "        )\n",
        "\n",
        "        result = self.call_llm(prompt)\n",
        "\n",
        "        memory_dto = AgentMemoryDTO(\n",
        "            agent_name=kwargs.get(\"agent_name\", self.name),\n",
        "            context=kwargs.get(\"context\", \"UnknownContext\"),\n",
        "            inputs=prompt,\n",
        "            output=result,\n",
        "            timestamp=datetime.now().isoformat(),\n",
        "        )\n",
        "\n",
        "        # Convert DTO to ORM instance\n",
        "        memory_entry = AgentMemory(**memory_dto.__dict__)\n",
        "        save_to_db(SessionAgentMemory, memory_entry)\n",
        "        logger.debug(\n",
        "            \"Memory saved for agent %s under context '%s'\",\n",
        "            self.name,\n",
        "            kwargs.get(\"context\", \"UnknownContext\"),\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Default task handler\n",
        "    # -------------------------------------------------------------------\n",
        "    def run_task(self, input_data: str) -> str:\n",
        "        \"\"\"Default generic task executor.\"\"\"\n",
        "        logger.info(\"Agent %s running task with input: %s\", self.name, input_data[:80])\n",
        "        prompt = f\"Process this as a {self.role}: {input_data}\"\n",
        "        return self.process(prompt)\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Send data between agents\n",
        "    # -------------------------------------------------------------------\n",
        "    def send_to(self, other_agent, input_data: str) -> str:\n",
        "        \"\"\"Send processed data from one agent to another.\"\"\"\n",
        "        logger.info(\n",
        "            \"%s → %s | Data length: %d\", self.name, other_agent.name, len(input_data)\n",
        "        )\n",
        "        return other_agent.run_task(input_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b589b07a",
      "metadata": {
        "id": "b589b07a"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Coordinator(Agent):\n",
        "    def __init__(self, model_type: str = \"mock\"):\n",
        "        super().__init__(\"Boss\", \"coordinator\", model_type)\n",
        "        self.team = []\n",
        "\n",
        "    def add_agent(self, agent):\n",
        "        \"\"\"Add an agent to the coordination team.\"\"\"\n",
        "        self.team.append(agent)\n",
        "        logger.info(f\"➕ Added {agent.name} to team.\")\n",
        "\n",
        "    def delegate_project(self, project_description):\n",
        "        \"\"\"Create a research plan and delegate tasks to all team members.\"\"\"\n",
        "        logger.info(f\"[{self.name}] Delegating project: {project_description}\")\n",
        "\n",
        "        # Generate plan\n",
        "        plan_prompt = f\"Create a step-by-step research plan for: {project_description}\"\n",
        "        plan_kwargs = {\"agent_name\": self.name, \"context\": \"delegate_project\"}\n",
        "\n",
        "        plan = self.process(plan_prompt, **plan_kwargs)\n",
        "        logger.info(f\"[{self.name}] Generated project plan.\")\n",
        "\n",
        "        # Delegate tasks to each team member\n",
        "        results = {}\n",
        "        for agent in self.team:\n",
        "            task = f\"Work on project '{project_description}' using your {agent.role} skills.\"\n",
        "            task_kwargs = {\"agent_name\": agent.name, \"context\": \"delegate_project\"}\n",
        "\n",
        "            logger.info(f\"[{self.name}] Assigning task to {agent.name} ({agent.role})\")\n",
        "            results[agent.name] = agent.process(task, **task_kwargs)\n",
        "            logger.info(f\"[{agent.name}] Task completed.\")\n",
        "\n",
        "        logger.info(\n",
        "            f\"[{self.name}] Delegation complete for project: {project_description}\"\n",
        "        )\n",
        "        return {\"plan\": plan, \"results\": results}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DecisionMaker(Agent):\n",
        "    def __init__(self, model_type: str = \"mock\"):\n",
        "        super().__init__(\"DecisionMaker\", \"investment decision analyst\", model_type)\n",
        "        logger.info(\"Initialized DecisionMaker agent\")\n",
        "\n",
        "    def make_decision(\n",
        "        self, query: str, senior_summary: str, analyst_summary: str\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Analyze the senior researcher's summary and analyst's insights to produce\n",
        "        actionable investment decisions.\n",
        "        Returns the raw model output as a string.\n",
        "        \"\"\"\n",
        "        logger.info(\"DecisionMaker analyzing query: %s\", query)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are an investment decision analyst.\n",
        "        Based on the following inputs, suggest an action for the query {query}.\n",
        "        Choose one of Buy, Hold, or Sell.\n",
        "        Provide a short rationale (1-2 sentences) and risk level (Low, Medium, High).\n",
        "\n",
        "        Senior Researcher's Summary:\n",
        "        {senior_summary}\n",
        "\n",
        "        Analyst's Insights:\n",
        "        {analyst_summary}\n",
        "        \"\"\"\n",
        "\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"make_decision\"}\n",
        "\n",
        "        decision = self.process(prompt, **kwargs)\n",
        "\n",
        "        logger.info(\"DecisionMaker completed decision for query: %s\", query)\n",
        "        logger.debug(\"DecisionMaker output: %s\", decision[:300])\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def run_task(self, input_data: tuple) -> str:\n",
        "        \"\"\"\n",
        "        Receive input data (query, senior_summary, analyst_summary)\n",
        "        and produce an investment decision.\n",
        "        \"\"\"\n",
        "        if not isinstance(input_data, (tuple, list)) or len(input_data) != 3:\n",
        "            logger.error(\"Invalid input format for DecisionMaker: %s\", input_data)\n",
        "            raise ValueError(\n",
        "                \"Expected input_data as a tuple: (query, senior_summary, analyst_summary)\"\n",
        "            )\n",
        "\n",
        "        query, senior_summary, analyst_summary = input_data\n",
        "        logger.debug(\"Running DecisionMaker task for query: %s\", query)\n",
        "        return self.make_decision(query, senior_summary, analyst_summary)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Evaluator(Agent):\n",
        "    def __init__(self, model_type: str = \"mock\"):\n",
        "        super().__init__(\"Evaluator\", \"evaluation agent\", model_type)\n",
        "        logger.info(\"Initialized Evaluator agent\")\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        query: str,\n",
        "        senior_summary: str,\n",
        "        analyst_summary: str,\n",
        "        decision_output: dict,\n",
        "    ) -> dict:\n",
        "        logger.info(\"Evaluator evaluating query: %s\", query)\n",
        "        prompt = f\"\"\"\n",
        "        You are an evaluator for investment research.\n",
        "        Evaluate the following for query: {query}:\n",
        "\n",
        "        Senior Researcher's Summary:\n",
        "        {senior_summary}\n",
        "\n",
        "        Analyst's Insights:\n",
        "        {analyst_summary}\n",
        "\n",
        "        Decision Maker's Suggestion:\n",
        "        {decision_output}\n",
        "\n",
        "        Check for:\n",
        "        - Completeness\n",
        "        - Clarity\n",
        "        - Accuracy\n",
        "        - Suggestions for improvement\n",
        "\n",
        "        Provide concise feedback.\n",
        "        \"\"\"\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"evaluate\"}\n",
        "        feedback_text = self.process(prompt, **kwargs)\n",
        "        return {\n",
        "            \"senior_summary_feedback\": feedback_text,\n",
        "            \"analyst_summary_feedback\": feedback_text,\n",
        "            \"decision_feedback\": feedback_text,\n",
        "        }\n",
        "\n",
        "    def run_task(self, input_data: tuple) -> dict:\n",
        "        return self.evaluate(*input_data)\n",
        "\n",
        "\n",
        "class Optimizer(Agent):\n",
        "    def __init__(self, model_type: str = \"mock\"):\n",
        "        super().__init__(\"Optimizer\", \"optimization agent\", model_type)\n",
        "        logger.info(\"Initialized Optimizer agent\")\n",
        "\n",
        "    def optimize(\n",
        "        self,\n",
        "        query: str,\n",
        "        senior_summary: str,\n",
        "        analyst_summary: str,\n",
        "        decision_output: dict,\n",
        "        evaluation_feedback: dict,\n",
        "    ) -> str:\n",
        "        logger.info(\"Optimizer optimizing query: %s\", query)\n",
        "        prompt = f\"\"\"\n",
        "        You are an optimizer for investment research.\n",
        "        Improve the summaries based on evaluation feedback:\n",
        "\n",
        "        Senior Summary: {senior_summary}\n",
        "        Analyst Insights: {analyst_summary}\n",
        "        Decision Output: {decision_output}\n",
        "        Evaluation Feedback: {evaluation_feedback}\n",
        "\n",
        "        Provide a refined summary or actionable recommendations.\n",
        "        \"\"\"\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"optimize\"}\n",
        "        return self.process(prompt, **kwargs)\n",
        "\n",
        "    def run_task(self, input_data: tuple) -> str:\n",
        "        query, senior_summary, analyst_summary, decision_output, evaluation_feedback = (\n",
        "            input_data\n",
        "        )\n",
        "        return self.optimize(\n",
        "            query, senior_summary, analyst_summary, decision_output, evaluation_feedback\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SeniorResearcher(Agent):\n",
        "    def __init__(self, model_type: str = \"mock\"):\n",
        "        super().__init__(\"SeniorResearcher\", \"lead researcher\", model_type)\n",
        "        self.stock_agent = StockAgent()\n",
        "        self.news_agent = NewsAgent()\n",
        "        self.economic_agent = EconomicAgent()\n",
        "\n",
        "    def select_research_source(self, query: str) -> list[str]:\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior researcher. Decide the best research source(s) for this query: {query}.\n",
        "        Options: Stock (ticker data), News (recent or historical headlines), Economic data (GDP, CPI, unemployment, etc.)\n",
        "        You can choose more than one. Return the selected sources as a comma-separated list.\n",
        "        \"\"\"\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"select_research_source\"}\n",
        "        choice_text = self.process(prompt, **kwargs)\n",
        "\n",
        "        sources = [c.strip().lower() for c in choice_text.split(\",\")]\n",
        "        valid_sources = {\"stock\", \"news\", \"economic\"}\n",
        "        selected_sources = [s for s in sources if s in valid_sources]\n",
        "        if not selected_sources:\n",
        "            selected_sources = [\"news\", \"economic\"]\n",
        "\n",
        "        logger.info(\n",
        "            \"Selected research sources for query '%s': %s\", query, selected_sources\n",
        "        )\n",
        "        return selected_sources\n",
        "\n",
        "    def research_stock(self, query: str) -> str:\n",
        "        logger.info(\"--- Senior Researcher Delegating Tasks for query: %s ---\", query)\n",
        "        selected_sources = self.select_research_source(query)\n",
        "        stock_summary, news_summary, economic_summary = \"\", \"\", \"\"\n",
        "\n",
        "        if \"stock\" in selected_sources:\n",
        "            stock_summary = self.stock_agent.run_task(query)\n",
        "\n",
        "        if \"news\" in selected_sources:\n",
        "            news_summary = self.news_agent.run_task(query)\n",
        "\n",
        "        if \"economic\" in selected_sources:\n",
        "            economic_prompt = f\"Provide economic indicators relevant to {query} and choose an appropriate timeframe.\"\n",
        "            economic_summary = self.economic_agent.run_task(economic_prompt)\n",
        "\n",
        "        raw_combined_text = \"\"\n",
        "        if stock_summary:\n",
        "            raw_combined_text += f\"Stock Summary:\\n{stock_summary}\\n\\n\"\n",
        "        if news_summary:\n",
        "            raw_combined_text += f\"News Summary:\\n{news_summary}\\n\\n\"\n",
        "        if economic_summary:\n",
        "            raw_combined_text += f\"Economic Summary:\\n{economic_summary}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior researcher. Based on the following summaries,\n",
        "        provide a concise, professional report highlighting key insights,\n",
        "        trends, risks, and potential opportunities for {query}.\n",
        "        {raw_combined_text}\n",
        "        \"\"\"\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"research_stock\"}\n",
        "        final_summary = self.process(prompt, **kwargs)\n",
        "\n",
        "        logger.info(\"--- Senior Researcher Final Summary for query '%s' ---\", query)\n",
        "        return final_summary\n",
        "\n",
        "    def run_task(self, input_data: str) -> str:\n",
        "        return self.research_stock(input_data)\n",
        "\n",
        "\n",
        "# --- Child Agents ---\n",
        "class StockAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"StockAgent\", \"stock data analyst\", \"mock\")\n",
        "        self.tool = StockTool()\n",
        "\n",
        "    def fetch_and_summarize(\n",
        "        self, symbol: str, period: str = \"1mo\", interval: str = \"1d\"\n",
        "    ) -> str:\n",
        "        logger.info(\"Fetching stock data for symbol: %s\", symbol)\n",
        "        try:\n",
        "            history = self.tool.fetch_historical_data(\n",
        "                symbol, period=period, interval=interval\n",
        "            )\n",
        "            history_text = \", \".join(\n",
        "                f\"{d}: {c}\"\n",
        "                for d, c in zip(\n",
        "                    list(history[\"Close\"].keys())[-10:],\n",
        "                    list(history[\"Close\"].values())[-10:],\n",
        "                )\n",
        "            )\n",
        "            financials = self.tool.fetch_financial_info(symbol)\n",
        "            info = self.tool.fetch_symbol_info(symbol)\n",
        "            financial_summary = f\"Market Cap: {info.get('market_cap')}, PE: {info.get('pe_ratio')}, Dividend Yield: {info.get('dividend_yield')}\"\n",
        "            prompt = f\"\"\"\n",
        "            You are a stock data analyst. Summarize the stock performance for {symbol} based on:\n",
        "            Recent closes: {history_text}\n",
        "            Financial metrics: {financial_summary}\n",
        "            \"\"\"\n",
        "            kwargs = {\"agent_name\": self.name, \"context\": \"fetch_and_summarize\"}\n",
        "            summary_text = self.process(prompt, **kwargs)\n",
        "            return summary_text\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Error fetching/summarizing data for %s: %s\", symbol, e)\n",
        "            return f\"Error fetching/summarizing data for {symbol}: {e}\"\n",
        "\n",
        "    def run_task(self, input_data: str) -> str:\n",
        "        return self.fetch_and_summarize(input_data)\n",
        "\n",
        "\n",
        "class NewsAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"NewsAgent\", \"financial news analyst\", \"mock\")\n",
        "        self.tool = NewsTool()\n",
        "\n",
        "    def select_news_source(self, query: str) -> str:\n",
        "        prompt = f\"\"\"\n",
        "        You are a financial news analyst. Decide the best news source for this query: {query}.\n",
        "        Options: recent, history, db. Return one word.\n",
        "        \"\"\"\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"select_news_source\"}\n",
        "        choice = self.process(prompt, **kwargs).strip().lower()\n",
        "        if choice not in [\"recent\", \"history\", \"db\"]:\n",
        "            choice = \"recent\"\n",
        "        logger.info(\"Selected news source for query '%s': %s\", query, choice)\n",
        "        return choice\n",
        "\n",
        "    def fetch_and_summarize(self, query: str, **kwargs) -> str:\n",
        "        source = self.select_news_source(query)\n",
        "        if \"q\" not in kwargs:\n",
        "            kwargs[\"q\"] = query\n",
        "        articles = self.tool.fetch_news(source=source, **kwargs)\n",
        "        raw_text = \"\"\n",
        "        for a in articles[:10]:\n",
        "            raw_text += f\"Title: {a.title}\\nDescription: {a.description}\\nPublished: {a.publishedAt}\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a financial news sentiment analyst.\n",
        "        Analyze the sentiment of the following news articles for the company/query '{query}'.\n",
        "        Provide a concise summary of overall sentiment (positive, negative, neutral),\n",
        "        highlight key drivers, and include examples from the articles:\n",
        "        {raw_text}\n",
        "        \"\"\"\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"fetch_and_analyze_sentiment\"}\n",
        "        sentiment_summary = self.process(prompt, **kwargs)\n",
        "        logger.info(\"Sentiment analysis complete for query '%s'\", query)\n",
        "        return sentiment_summary\n",
        "\n",
        "    def run_task(self, input_data: str) -> str:\n",
        "        return self.fetch_and_summarize(input_data)\n",
        "\n",
        "\n",
        "class EconomicAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"EconomicAgent\", \"economic data analyst\", \"mock\")\n",
        "        self.tool = EconomicDataTool()\n",
        "\n",
        "    def fetch_and_summarize(\n",
        "        self, series_ids: list, start_date: str = None, end_date: str = None\n",
        "    ) -> str:\n",
        "        raw_text = \"\"\n",
        "        for series in series_ids:\n",
        "            try:\n",
        "                observations = self.tool.fetch_series(series, start_date, end_date)\n",
        "                if observations:\n",
        "                    obs_text = \", \".join(\n",
        "                        f\"{obs['date']}:{obs['value']}\" for obs in observations[-10:]\n",
        "                    )\n",
        "                    raw_text += f\"{series}: {obs_text}\\n\"\n",
        "            except Exception as e:\n",
        "                logger.exception(\"Error fetching economic data for %s: %s\", series, e)\n",
        "                raw_text += f\"{series}: Error fetching data\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are an economic data analyst. Summarize this economic data in a concise, professional format:\n",
        "        {raw_text}\n",
        "        \"\"\"\n",
        "        summary_text = self.process(prompt)\n",
        "        logger.info(\"Economic summarization complete for series: %s\", series_ids)\n",
        "        return summary_text\n",
        "\n",
        "    def run_task(self, input_data) -> str:\n",
        "        series_ids = input_data if isinstance(input_data, list) else [input_data]\n",
        "        return self.fetch_and_summarize(series_ids)\n",
        "\n",
        "\n",
        "\n",
        "class Analyst(Agent):\n",
        "    def __init__(self, model_type: str = \"mock\"):\n",
        "        super().__init__(\"Analyst\", \"financial analyst\", model_type)\n",
        "\n",
        "    def run_task(self, research_summary: str) -> str:\n",
        "        \"\"\"\n",
        "        Receive input from Researcher and provide analysis insights.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are a financial analyst. Based on the following research summaries,\n",
        "        provide insights on trends, risks, and potential opportunities:\n",
        "\n",
        "        {research_summary}\n",
        "        \"\"\"\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"run_task\"}\n",
        "\n",
        "        logger.info(f\"[{self.name}] Starting analysis with context: {kwargs}\")\n",
        "        analysis = self.process(prompt, **kwargs)\n",
        "        logger.info(f\"[{self.name}] Analysis complete. Output: {analysis[:200]}...\")\n",
        "\n",
        "        return analysis\n",
        "\n",
        "class Writer(Agent):\n",
        "    def __init__(self, model_type: str = \"mock\"):\n",
        "        super().__init__(\"Writer\", \"content writer\", model_type)\n",
        "        logger.info(\"Initialized Writer agent\")\n",
        "\n",
        "    def write_report(self, content: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a professional report based on the given content.\n",
        "        \"\"\"\n",
        "        logger.info(\"Writer received content to generate report\")\n",
        "        kwargs = {\"agent_name\": self.name, \"context\": \"write_report\"}\n",
        "\n",
        "        report = self.process(\n",
        "            f\"Write a professional report based on this analysis:\\n{content}\", **kwargs\n",
        "        )\n",
        "\n",
        "        logger.info(\"Writer completed report generation\")\n",
        "        logger.debug(\"Report content (truncated 300 chars): %s\", report[:300])\n",
        "        return report\n",
        "\n",
        "    def run_task(self, input_data: str) -> str:\n",
        "        logger.info(\"Writer running task\")\n",
        "        return self.write_report(input_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4510a055",
      "metadata": {
        "id": "4510a055"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiAgentTeam:\n",
        "    def __init__(self):\n",
        "        # Instantiate agents\n",
        "        self.coordinator = Coordinator()\n",
        "        self.researcher = SeniorResearcher()\n",
        "        self.analyst = Analyst()\n",
        "        self.decision_maker = DecisionMaker()\n",
        "        self.evaluator = Evaluator()\n",
        "        self.optimizer = Optimizer()\n",
        "        self.writer = Writer()\n",
        "\n",
        "        # Wire the team\n",
        "        self.coordinator.add_agent(self.researcher)\n",
        "        self.coordinator.add_agent(self.analyst)\n",
        "        self.coordinator.add_agent(self.decision_maker)\n",
        "        self.coordinator.add_agent(self.evaluator)\n",
        "        self.coordinator.add_agent(self.optimizer)\n",
        "        self.coordinator.add_agent(self.writer)\n",
        "\n",
        "\n",
        "        logger.info(\"Multi-agent team created!\")\n",
        "\n",
        "    def execute_project(self, project_description: str):\n",
        "        logger.info(f\"EXECUTING PROJECT: {project_description}\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        result = {}\n",
        "        try:\n",
        "            # Step 1: Coordinator delegates project\n",
        "            result[\"coordination\"] = self.coordinator.delegate_project(\n",
        "                project_description\n",
        "            )\n",
        "            print(result[\"coordination\"])\n",
        "        except Exception as e:\n",
        "            result[\"coordination\"] = f\"[Error] {e}\"\n",
        "            logger.error(\"Coordinator error: %s\", e)\n",
        "\n",
        "        try:\n",
        "            # Step 2: SeniorResearcher gathers research\n",
        "            result[\"research\"] = self.researcher.research_stock(project_description)\n",
        "        except Exception as e:\n",
        "            result[\"research\"] = f\"[Error] {e}\"\n",
        "            logger.error(\"Researcher error: %s\", e)\n",
        "\n",
        "        try:\n",
        "            # Step 3: Analyst processes the research\n",
        "            analyst_inputs = result.get(\"research\", \"\")\n",
        "            result[\"analysis\"] = self.researcher.send_to(self.analyst, analyst_inputs)\n",
        "        except Exception as e:\n",
        "            result[\"analysis\"] = f\"[Error] {e}\"\n",
        "            logger.error(\"Analyst error: %s\", e)\n",
        "\n",
        "        try:\n",
        "            # Step 4: DecisionMaker generates decision/recommendation\n",
        "            inputs = (\n",
        "                project_description,\n",
        "                result.get(\"research\", \"\"),\n",
        "                result.get(\"analysis\", \"\"),\n",
        "            )\n",
        "            result[\"decision\"] = self.analyst.send_to(self.decision_maker, inputs)\n",
        "        except Exception as e:\n",
        "            result[\"decision\"] = f\"[Error] {e}\"\n",
        "            logger.error(\"DecisionMaker error: %s\", e)\n",
        "\n",
        "        try:\n",
        "            # Step 5a: Evaluator reviews the outputs\n",
        "            eval_inputs = (*inputs, result.get(\"decision\", \"\"))\n",
        "            evaluation_feedback = self.decision_maker.send_to(\n",
        "                self.evaluator, eval_inputs\n",
        "            )\n",
        "\n",
        "            # Step 5b: Optimizer refines summaries based on evaluation\n",
        "            opt_inputs = (*inputs, result.get(\"decision\", \"\"), evaluation_feedback)\n",
        "            result[\"raw_output\"] = self.evaluator.send_to(\n",
        "                self.optimizer, opt_inputs\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            result[\"raw_output\"] = f\"[Error] {e}\"\n",
        "            logger.error(\"Evaluator or Optimizer error: %s\", e)\n",
        "\n",
        "        try:\n",
        "            # Step 6: Writer agent\n",
        "            result[\"report\"] = self.optimizer.send_to(\n",
        "                self.writer, result[\"raw_output\"]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            result[\"report\"] = f\"[Error] {e}\"\n",
        "            logger.error(\"Writer error: %s\", e)\n",
        "        logger.info(\"PROJECT EXECUTION COMPLETED!\")\n",
        "        return result\n",
        "\n",
        "    def show_team_status(self):\n",
        "        logger.info(\"TEAM STATUS\")\n",
        "        agents = [\n",
        "        self.coordinator,\n",
        "        self.researcher,\n",
        "        self.analyst,\n",
        "        self.decision_maker,\n",
        "        self.evaluator,\n",
        "        self.optimizer,\n",
        "        self.writer\n",
        "    ]\n",
        "\n",
        "        for agent in agents:\n",
        "            tasks_completed = len(getattr(agent, \"memory\", {}))\n",
        "            logger.info(\n",
        "                \"  %s (%s): %d tasks completed\",\n",
        "                agent.name,\n",
        "                getattr(agent, \"role\", \"N/A\"),\n",
        "                tasks_completed,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9c22d675",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c22d675",
        "outputId": "99d02830-0f05-4d95-9bf3-9c9060f50b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Request failed for endpoint https://newsapi.org/v2/top-headlines: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/top-headlines?q=Analyze+AAPL&apiKey=NEWS_API_KEY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'plan': 'Mock response from Boss: Create a step-by-step research plan for: Analyze A...', 'results': {'SeniorResearcher': \"Mock response from SeniorResearcher: Work on project 'Analyze AAPL' using your lead res...\", 'Analyst': \"Mock response from Analyst: Work on project 'Analyze AAPL' using your financia...\", 'DecisionMaker': \"Mock response from DecisionMaker: Work on project 'Analyze AAPL' using your investme...\", 'Evaluator': \"Mock response from Evaluator: Work on project 'Analyze AAPL' using your evaluati...\", 'Optimizer': \"Mock response from Optimizer: Work on project 'Analyze AAPL' using your optimiza...\", 'Writer': \"Mock response from Writer: Work on project 'Analyze AAPL' using your content ...\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Request failed for series Provide economic indicators relevant to Analyze AAPL and choose an appropriate timeframe.: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=Provide+economic+indicators+relevant+to+Analyze+AAPL+and+choose+an+appropriate+timeframe.&api_key=FRES_API_KEY&file_type=json\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3110710531.py\", line 189, in fetch_series\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=Provide+economic+indicators+relevant+to+Analyze+AAPL+and+choose+an+appropriate+timeframe.&api_key=FRES_API_KEY&file_type=json\n",
            "ERROR:__main__:Error fetching economic data for Provide economic indicators relevant to Analyze AAPL and choose an appropriate timeframe.: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=Provide+economic+indicators+relevant+to+Analyze+AAPL+and+choose+an+appropriate+timeframe.&api_key=FRES_API_KEY&file_type=json\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1644020895.py\", line 340, in fetch_and_summarize\n",
            "    observations = self.tool.fetch_series(series, start_date, end_date)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3110710531.py\", line 189, in fetch_series\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=Provide+economic+indicators+relevant+to+Analyze+AAPL+and+choose+an+appropriate+timeframe.&api_key=FRES_API_KEY&file_type=json\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize DB\n",
        "    init_db()\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    team = MultiAgentTeam()\n",
        "    project_result = team.execute_project(\"Analyze AAPL\")\n",
        "    team.show_team_status()\n",
        "\n",
        "    logger.info(\"Project Result Summary:\")\n",
        "    for k, v in project_result.items():\n",
        "        logger.info(\"%s: %s\", k, v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# WORKFLOW OVERVIEW\n",
        "# ============================================================\n",
        "\n",
        "# Coordinator → SeniorResearcher → (StockAgent / NewsAgent / EconomicAgent)\n",
        "# → Analyst → DecisionMaker → Evaluator → Optimizer → Writer\n",
        "\n",
        "# Flow Explanation:\n",
        "# 1. Coordinator: Plans research steps for a given stock symbol.\n",
        "# 2. SeniorResearcher: Routes the task to the right specialists.\n",
        "# 3. StockAgent / NewsAgent / EconomicAgent: Fetch and process data.\n",
        "# 4. Analyst: Analyzes research summaries and identifies insights.\n",
        "# 5. DecisionMaker: Generates actionable investment decisions.\n",
        "# 6. Evaluator: Evaluates the quality and coherence of decisions.\n",
        "# 7. Optimizer: Refines decisions using evaluator feedback.\n",
        "# 8. Writer: Compiles the final investment report."
      ],
      "metadata": {
        "id": "Hn2dLMopfWR-"
      },
      "id": "Hn2dLMopfWR-",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f7e380a6",
      "metadata": {
        "id": "f7e380a6"
      },
      "outputs": [],
      "source": [
        "# Agent Functions:\n",
        "# 1. Plans its research steps for a given stock symbol:\n",
        "#    - The `Coordinator` agent's `delegate_project` method generates a research plan. (See `Coordinator` class in the code.)\n",
        "# 2. Uses tools dynamically (APIs, datasets, retrieval):\n",
        "#    - The `StockAgent` uses `StockTool` (yfinance).\n",
        "#    - The `NewsAgent` uses `NewsTool` (NewsAPI and local DB).\n",
        "#    - The `EconomicAgent` uses `EconomicDataTool` (FRED API). (See `StockAgent`, `NewsAgent`, and `EconomicAgent` classes.)\n",
        "# 3. Self-reflects to assess the quality of its output:\n",
        "#    - The `Evaluator` agent's `evaluate` method assesses the quality of the research output. (See `Evaluator` class.)\n",
        "# 4. Learns across runs (e.g., keeps brief memories or notes to improve future analyses):\n",
        "#    - The `Agent` base class includes a `process` method that saves inputs, outputs, and context to the `AgentMemory` database. (See `Agent` class and the database setup.)\n",
        "\n",
        "# Workflow Patterns:\n",
        "# 1. Prompt Chaining: Ingest News -> Preprocess -> Classify -> Extract -> Summarize\n",
        "#    - This pattern is demonstrated in the `NewsAgent`'s `fetch_and_summarize` method, which fetches articles, processes the raw text, and then uses a prompt to analyze sentiment and summarize. (See `NewsAgent` class.)\n",
        "# 2. Routing: Direct content to the right specialist (e.g., earnings, news, or market analyzers)\n",
        "#    - The `SeniorResearcher` agent's `select_research_source` method determines which child agents (`StockAgent`, `NewsAgent`, `EconomicAgent`) are relevant to the query and routes the task accordingly. (See `SeniorResearcher` class.)\n",
        "#    - The `MultiAgentTeam` orchestrates the flow between the `SeniorResearcher`, `Analyst`, `DecisionMaker`, `Evaluator`, `Optimizer`, and `Writer` agents. (See `MultiAgentTeam` class.)\n",
        "# 3. Evaluator–Optimizer: Generate analysis -> evaluate quality -> refine using feedback.\n",
        "#    - This pattern is implemented through the interaction of the `DecisionMaker` (generates decision), `Evaluator` (evaluates the decision and summaries), and `Optimizer` (refines output based on feedback). (See `MultiAgentTeam`'s `execute_project` method and the `Evaluator` and `Optimizer` classes.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ffe0d625",
      "metadata": {
        "id": "ffe0d625"
      },
      "outputs": [],
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "FinalProject (3.11.14)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}